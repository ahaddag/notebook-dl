{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:22pt; line-height:25pt; font-weight:bold; text-align:center;\">On The Capacity Of Feedforward Neural Networks</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vue d'Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout au long de ce notebook, nous allons définir et aborder la notion de **capacité** pour les réseaux de neurones denses, tout en apportant des illustrations et des exemples concrets sous forme de courts exercices pour aider à la compréhension. Nous allons traiter un résultat central sur la capacité de ces réseaux de neurones en lien avec les fonctions binaires *et nous terminerons par un tour d'horizon sur les CNN*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Sommaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capacité : Définition et Interprétation <a id=\"capacité--définition-et-interprétation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous utiliserons les notations suivantes pour définir nos réseaux de neurones : <br>\n",
    "Soit un réseau de neurones à $L$ couches. On notera $n_k$ le nombre de noeuds à la couche $k$. <br>\n",
    "Le réseau de neurones sera alors noté $A(n_1,...,n_L)$. \n",
    "\n",
    "<img src=\"img/nn.png\" width=\"800px\"></img> <br>\n",
    "Par exemple, le réseau de neurone ci-dessus peut être noté $A(5, 12, 10, 1)$. \n",
    "\n",
    "$n_1$ correspond à la couche d'entrée et $n_L$ à celle de sortie, on peut donc considérer qu'il a $L-2$ couches cachées. On supposera durant tout le notebook que le réseau est **dense** (i.e tous les noeuds d'une couches sont connctés à tous ceux de la couche suivante) et que toutes les fonctions d'activations sont des fonctions de *Heaviside* de la forme $f (x) = h(\\langle a, x\\rangle + α)$ où $h(t)=1$ si $t > 0$ et $h(t)=0$ sinon, qu'on appelle aussi Perceptron de Rosenblatt.\n",
    "\n",
    "Nous utiliserons la notation $a \\asymp b$ d'égalité asymptotique qui signifie : $\\exists c_1, c_2 > 0$ tq. $c_1 b \\leq a \\leq c_2 b$ <br>\n",
    "Nous noterons $H^n = \\{0,1\\}^n$ l'hypercube booléen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Théorème d'Approximation Universelle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rappelons l'énoncé informel du théorème d'approximation universelle : \n",
    "\n",
    "Si $\\sigma$ est une fonction d'activation non-constante, continue, bornée et monotone croissante, alors un réseau de la forme $A(m, N, n)$ peut approcher n'importe quelle fonction continue avec une précision arbitraire à condition que N puisse être arbitrairement grand. \n",
    "\n",
    "Plus de précisions peuvent être apportées : [Wikipedia](https://fr.wikipedia.org/wiki/Th%C3%A9or%C3%A8me_d%27approximation_universelle) et <a href=\"../docs/UniversalApproximationTheorem.pdf\">Rappel du théorème vu en classe de DL.</a>\n",
    "\n",
    "Ce théorème stipule qu'**en  théorie**, et sous certaines hypothèses, n'importe quelle fonction continue peut être approchée par un réseau dense à une couche cachée. Mais **en pratique**, les réseaux de neurones ont souvent plusieurs couches cachées, et certaines hypothèses ne sont plus respectées (surtout celle sur la continuité de la fonction à approcher, on préfererait approcher des fonctions plus moches...). Le notion de **capacité** permet justement de caractériser cette qualité d'approximation dans un cadre plus général et pour des réseaux de neurones à $L$ couches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La **capacité** d'un réseau de neurones de type $A(n_1, n_2,..., n_L)$ notée $C(A)$ est définie comme le logarithme binaire du nombre de fonctions $f : H^{n_1} \\rightarrow H^{n_L}$ que le réseau est capable de calculer. <br>\n",
    "<div class=\"alert alert alert-warning\"> \n",
    "\n",
    "**Exercice :** Sans considérations supplémentaires, quelle est la capacité maximale théorique qu'un réseau $A(n_1, n_2,..., n_L)$ ? <br>\n",
    "On s'appuiera simplement sur le dénombrement des fonctions $f : H^{n_1} \\rightarrow H^{n_L}$\n",
    "\n",
    "</div>\n",
    "\n",
    "<details class=\"alert alert-danger\">\n",
    "    <summary markdown=\"span\"><b>Solution (cliquez pour étendre la cellule)</b></summary>\n",
    "\n",
    "En notant $N$ (resp. $M$) le nombre d'éléments dans $H^{n}$ (resp. $H^{m}$), il y a $N^M$ fonctions allant de $H^{m}$ dans $H^{n}$. Si l'on se donne un vecteur de $H^{n}$, chaque élément vaudra 0 ou 1, il y a donc 2 choix possibles par élément du vecteur, soit $2^n$ possibilités. Le méme raisonnement s'applique pour $H^{m}$, ce qui donne un nombre total de ${(2^n)}^{2^m} = 2^{n 2^m}$ fonctions. On a alors une capacité maximale établie à $C(A) = \\log_2(2^{n_L 2^{n_1}}) = n_L 2^{n_1}$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien sûr, la capacité maximale calculée au-dessus est bien trop grande pour être utile (faire le calcul du nombre de fonctions allant de $H^{5}$ dans $H^{2}$) et on souhaiterait obtenir des bornes plus fines pour encadrer la capacité d'un réseau de neurone. Mais d'abord, que signifie concrètement la capacité ?<br>\n",
    "\n",
    "La capacité d'un réseau de neurones peut être vue, au-delà du nombre de fonctions binaires que le réseau peut calculer, comme un indicateur de la *complexité* des fonctions qu'il est capable d'approcher fidèlement dans un problème de regression, ou la *complexité* des régions qu'il est capable de séparer dans un problème de classification.<br>\n",
    "La capacité d'un réseau de neurones est également liée à la notion d'*overfitting* et d'*underfitting* : une capacité grande caractérise une tendance à plutôt sur-apprendre et inversement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Résultat principal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"../docs/the_capacity_of_feedforward_neural_networks.pdf\">P. Baldi et R. Vershynin</a> offrent un résultat remarquable autour de la capacité des réseaux de neurones denses :<br>\n",
    "\n",
    "Soit un réseau de neurones $A(n_1,...,n_L)$ tq. $\\forall$ $1\\leq j\\leq k\\leq L,$ $n_j > 18\\log_2{(L n_k)}$ :\n",
    "\n",
    "<div style=\"text-align:center; font-size:20pt;\"> \n",
    "\n",
    "$C(A) \\asymp \\sum_{k=1}^{L-1}\\min(n_1,\\dots,n_k)n_kn_{k+1}$\n",
    "</div>\n",
    "\n",
    "Pour rappel, il existe donc deux constantes strictement positives $c_1$ et $c_2$ telles que $c_1\\sum_{k=1}^{L-1}\\min(n_1,\\dots,n_k)n_kn_{k+1} \\leq C(A) \\leq c_2\\sum_{k=1}^{L-1}\\min(n_1,\\dots,n_k)n_kn_{k+1}$ et il a même été prouvé que $c_2=1$ et donc $c_1 \\in$ $]0;1[$. Plusieurs informations peuvent être tirées de cette formule :\n",
    "- La capacité d'un réseau de neurones dépend de $L$ le nombre de couches, un réseau plus profond aura une capacité plus grande à nombre de noeuds par couche équivalent.\n",
    "- La présence d'un \"goulot d'étranglement\" (une couche avec peu de neurones) réduit fortement la capcité, d'autant plus si il est situé parmi les premières couches du réseau.\n",
    "- Pour refaire le lien avec le théorème d'approximation universelle, pour un réseau $A(m, N, n)$ (qui est un cas particulier et qui ne nécessite pas les mêmes hypothèses que le résultat précédent), si N est suffisamment grand, on a $C(A) \\asymp m^2N$ et donc $C(A) \\to +\\infty$ lorsque $N \\to +\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Éléments de Preuve <a id=\"éléments-de-preuve\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
